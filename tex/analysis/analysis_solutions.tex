\documentclass[answers,12pt]{exam}
\usepackage[utf8]{inputenc, xcolor}
\usepackage{hyperref}
\usepackage{amsmath,amsthm,amssymb,empheq}
\usepackage{float}
\usepackage{todonotes}

\newcommand{\increment}{\text{++}}
\newcommand{\fakeminus}{\text{ --- }}
\newcommand{\suchthat}{\text{ s.t. }}

\title{Analysis Solutions}
\author{Ashwin Sreevatsa}
\date{April 2023}

\begin{document}

\maketitle

\section*{Quick Remarks}
This will be a solution page for problems in Terence Tao's Analysis 1 (Fourth edition).
The problems I'm adding here are those that I worked on and solved myself.
I don't expect to add all the problems in the textbook, but rather those that I find the most interesting.
I will try and do a few problems from each section though, just to make sure I'm actually understanding the concepts properly.

\section{Introduction}

\section{Starting at the Beginning: The Natural Numbers}

\textbf{Proposition 2.1.6:} 4 is not equal to 0 
\todo{Add column proofs.}

\begin{solution}
    \[
        \begin{aligned} 
            &4 = 3\increment \text{ (definition 2.1.3)} \\
            &3\increment \text{ is a natural number (axiom 2.1, axiom 2.2).} \\
            &3\increment \neq 0 \text{ (axiom 2.3)}
        \end{aligned}
    \]
\end{solution}

\textbf{Proposition 2.1.8:} 6 is not equal to 2
\begin{solution}
    \[
        \begin{aligned}
            &6=2\\
            \implies &5 \increment = 1 \increment \text{ (axiom 2.2, def 2.1.3)}\\
            \implies &5 = 1 \text{ (axiom 2.4)}\\
            \implies &4 \increment = 0 \increment \text{ (axiom 2.2, def 2.1.3)}\\
            \implies &4 = 0 \text{ (axiom 2.4)}\\
        \end{aligned}
    \]
    We know that this is false from prop 2.1, so $6 \neq 2$.
\end{solution}

\textbf{Proposition 2.1.16:}  

\begin{solution}

\end{solution}

\textbf{Lemma 2.2.2:}  $n+0=n$

\begin{solution}
We use induction.
Let property $P(n)$ be that $n+0=n$.
$P(0): 0+0=0$, which holds by the definition of addition (1).
We want to show that $P(n) \implies P(n \increment)$.
If $n+0=n$ by the inductive hypothesis, then 
\begin{align*}
    (n \increment) + 0 &= (n+0) \increment && \text{def of addition 2}\\
    &= n \increment && \text{inductive hypothesis}
\end{align*}
So $P(n)$ holds for all natural numbers $n$.
\end{solution}

\textbf{Lemma 2.2.3:}  $n+ (m \increment) = (n+m) \increment$

\begin{solution}
Let $P(n): n+ (m \increment) = (n+m) \increment$.
Base case: 
\begin{align*}
    P(0): 0+ (m \increment) &= m \increment && (1) \\
    &= (0 + m) \increment && (1)
\end{align*}
Inductive step: $P(n) \implies P(n \increment)$
\begin{align*}
    (n \increment) + (m \increment) &= (n+ (m \increment)) \increment && (2) \\
    &= ((n+m) \increment) \increment && \text{inductive step} \\
    &= ((n \increment) + m) \increment && (2) \\
\end{align*}
By axiom 2.5, $P(n)$ holds for all natural numbers.
\end{solution}

\textbf{Corollary 2.2.3.1} (Technically not an actual problem, just a note): $n \increment = n+1$

\begin{solution}
    \begin{align*}
        n \increment &= (n+0) \increment && \text{Lemma 2.2.2} \\
        &= n + (0 \increment) && \text{Lemma 2.2.3}\\
        &= n+1 && \text{Definition of 1}
    \end{align*}
\end{solution}

\textbf{Prop 2.2.4} Commutativity of natural numbers in addition: $P(n): n+m = m+n$

\begin{solution}
    We use induction to show property $P(n): n + m = m + n$

    Base case $P(0)$
    \begin{align*}
        0+m &= m && (1) \\
        &= m+0 && \text{(lemma 2.2.2)}
    \end{align*}
    Inductive step $P(n) \implies P(n\increment)$:
    \begin{align*}
        (n \increment) + m &= (n+m) \increment && (2) \\
        &= (m+n) \increment && \text{(inductive hyp)} \\
        &= m+ (n \increment) && \text{(lemma 2.2.3)}
    \end{align*}
    By axiom 2.5, $P(n)$ holds for all natural numbers.
\end{solution}

\textbf{Prop 2.2.5} Commutativity of addition over natural numbers

\begin{solution}
    Let $P(c):= (a+b)+c = a+(b+c)$, with $a,b,c$ as natual numbers.
    We use induction to prove this.
    
    Base case: $P(0)$:
    \begin{align*}
        (a+b)+0 &= a+b && \text{lemma 2.2.2}\\
        &= a+(b+0) && \text{lemma 2.2.2}
    \end{align*}

    Inductive hypothesis: $P(n) \implies P(n \increment)$
    \begin{align*}
        (a+b)+ c \increment &= ((a+b)+c) \increment && \text{lemma 2.2.3} \\
        &=(a+(b+c))\increment && \text{inductive hypothesis} \\
        &= a + (b+c) \increment && \text{lemma 2.2.3} \\
        &= a+(b+c \increment) && \text{lemma 2.2.3}
    \end{align*}
    By axiom 2.5 (or whatever), this holds for all natural numbers and we are done.
\end{solution}

\textbf{Prop 2.2.6} Additive cancellation law

\begin{solution}
    Let's say $P(a): a+b = a+c \implies b=c$.
    We want to show that this property holds via induction.

    Base case $P(0)$: we have $0+b=0+c$.
    Then:
    \begin{align*}
        b &= 0+b && (1) \\
        &= 0+c && \text{assumption} \\ 
        &= c && (1)
    \end{align*}

    Inductive step $P(n) \implies P(n \increment)$.

    We have that $a+b=a+c \implies b=c$ and $(a \increment) +b  = (a \increment) + c$.
    \begin{align*}
        (a + b) \increment &= (a \increment) + b && (2) \\
        &= (a \increment) + c && \text{assumption} \\ 
        &= (a+c) \increment && (2)
    \end{align*}
    We then have $a+b=a+c$ from axiom 2.4 $(a \increment = b \increment \implies a=b)$.

    By axiom 2.5, this holds for all natural numbers.
\end{solution}

\section{Set Theory}

\section{Integers and Rationals}
\subsection{4.1 Integers}
\textbf{Lemma 4.1.3} Addition and multiplication are well-defined.
Let $a,b,a',b',c,d$ be natural numbers.
If $(a \fakeminus b)=(a' \fakeminus b')$, then $(a \fakeminus b)+(c \fakeminus d)=(a' \fakeminus b')+(c \fakeminus d)$ and $(a \fakeminus b)\times(c \fakeminus d)=(a' \fakeminus b')\times(c \fakeminus d)$.
(Note that \fakeminus{} is not the same as subtraction since subtraction hasn't yet been defined.)

\begin{solution}
\begin{proof}
    To show the first part:
    \begin{align*}
        (a \fakeminus b) + (c \fakeminus d) &= (a' \fakeminus b') + (c \fakeminus d)\\
        \iff (a+c) \fakeminus (b+d) &= (a' + c) \fakeminus (b' + d) && \text{(Def 4.1.2)}\\
        \iff (a+c) + (b'+d) &= (a' + c) + (b + d)  && \text{(Def 4.1.1)}\\
        \iff (a+b') &= (a'+b)  && \text{(Asso., comm., prop 2.2.6)}\\
        \iff (a \fakeminus b) &= (a' \fakeminus b')  && \text{(Def 4.1.1)}
    \end{align*}
    To show the second part, note that we want $(a \fakeminus b) = (a' \fakeminus b') \implies (a \fakeminus b) \times (c \fakeminus d)  = (a' \fakeminus b') \times (c \fakeminus d)$:
    \begin{align*}
        (a \fakeminus b) \times (c \fakeminus d) &= (a' \fakeminus b') \times (c \fakeminus d) \\
        \iff (ac + bd) \fakeminus (ad + bc) &= (a'c + b'd) \fakeminus (a'd + b'c)&& \text{(Def 4.1.2)} \\
        \iff (ac + bd) + (a'd + b'c) &= (a'c + b'd) + (ad + bc) && \text{(Def 4.1.1)}\\
        \iff (a+b')c + (a'+b)d &= (a'+b)c + (a+b')d && \text{(Asso., comm., dist.)}
    \end{align*}
    But clearly this last statement is true from the assumption that $(a \fakeminus b) = (a' \fakeminus b') \iff (a + b') = (a' + b)$ and substituting the terms as needed.
\end{proof}

Note for posterity: originally I was attempting to prove the second part by showing that $(a \fakeminus b) \times (c \fakeminus d) = (a' \fakeminus b') \times (c \fakeminus d) \iff (a \fakeminus b) = (a' \fakeminus b')$ similar to the first part.
This is a false statement in general (just take $c = d$), yet it didn't occur to me to question it until I got repeatedly stuck.
\end{solution}

\textbf{Lemma 4.1.3} (Laws of algebra for addition)

\begin{solution}
\begin{proof}
    Note that for most of these, we take $x = a \fakeminus b$, where $x$ is an integer and $a,b$ are natural numbers.

    Reflexivity:
    \begin{align*}
        x+y &= (a \fakeminus b) + (c \fakeminus d) \\
        &= (a+c) \fakeminus (b+d) \\
        &= (c+a) \fakeminus (d+b) \\
        &= (c \fakeminus d) + (a \fakeminus b) \\
        &= y+x
    \end{align*}

    Associativity
    \begin{align*}
        (x+y)+z &= ((a \fakeminus b) + (c \fakeminus d)) + (e \fakeminus f) \\
        &= ((a+c) \fakeminus (b+d)) + (e \fakeminus f) \\
        &= ((a+c)+e) \fakeminus ((b+d)+f) \\
        &= (a+(c+e)) \fakeminus (b+(d+f)) \\
        &= (a \fakeminus b) + ((c+e) \fakeminus (d+f)) \\
        &= (a \fakeminus b) + ((c \fakeminus d) + (e \fakeminus f)) \\
        &= x + (y+z)
    \end{align*}

    Additive identity
    \begin{align*}
        x+0 &= (a \fakeminus b) + 0 \\
        &= (a \fakeminus b) + (0 \fakeminus 0) \\
        &= (a+0) \fakeminus (b+0) \\
        &= a \fakeminus b \\
        &= x
    \end{align*}
    \begin{align*}
        x+0 &= (a \fakeminus b) + 0 \\
        &= (a \fakeminus b) + (0 \fakeminus 0) \\
        &= (a+0) \fakeminus (b+0) \\
        &= (0+a) \fakeminus (0+b) \\
        &= 0 + x
    \end{align*}

    Additive inverse
    \begin{align*}
        x + (-x) &= (a \fakeminus b) + (- (a \fakeminus b)) \\
        &= (a \fakeminus b) + (b \fakeminus a) \\
        &= (a+b) \fakeminus (b+a) \\
        &= (a+b) \fakeminus (a+b) \\ 
        &= 0 \\
        (-x)+x &= x + (-x) && \text{commutativity of integers}
    \end{align*}

    Multiplicative commutativity
    \begin{align*}
        xy &= (a \fakeminus b)(c \fakeminus d) \\
        &= (ac + bd) \fakeminus (ad + bc) \\
        &= (ca + db) \fakeminus (da + cb) \\ 
        &= (c \fakeminus d)(a \fakeminus b) \\
        &= yx
    \end{align*}

    Multiplicative associativity
    \begin{align*}
        (xy)z &= ((a \fakeminus b)(c \fakeminus d))(e \fakeminus f) \\
        &= ((ac + bd) \fakeminus (ad + bc))(e \fakeminus f) \\
        &= ((ac + bd)e + (ad + bc)f) \fakeminus ((ad + bc)e + (ac + bd)f) \\ 
        &= (ace + bde + bcf + adf) \fakeminus (bce + ade + acf + bdf) \\
        &= (a \fakeminus b)((ce + df) \fakeminus (cf + de)) \\
        &= (a \fakeminus b)((c \fakeminus d)(e \fakeminus f)) \\
        &= x(yz)
    \end{align*}

    Multiplicative identity
    \begin{align*}
        x \cdot 1 &= (a \fakeminus b) \cdot 1 \\
        &= (a \fakeminus b)(1 \fakeminus 0) \\
        &= (a \cdot 1 + b \cdot 0) \fakeminus (a \cdot 0 + b \cdot 1) \\
        &= (a \fakeminus b) \\
        &= x \\
        x \cdot 1 &= 1 \cdot x
    \end{align*}
    
    Distributive property
    \begin{align*}
        x(y+z) &= (a \fakeminus b)((c \fakeminus d) + (e \fakeminus f)) \\
        &= (a \fakeminus b) ((c+e) \fakeminus (d+f)) \\\
        &= (a(c+e) + b(d+f)) \fakeminus (b(c+e) + a(d+f)) \\
        &= (ac + bd + ae + bf) \fakeminus (bc + be + ad + af) \\
        &= (ac + bd + ae + bf) \fakeminus (be + af + ad + bc) \\
        &= ((ac +bd) \fakeminus (ad + bc)) + ((ae + bf) \fakeminus (be + af)) \\
        &= (a \fakeminus b)(c \fakeminus d) + (a \fakeminus b)(e \fakeminus f) \\
        &= xy + xz
    \end{align*}
\end{proof}
\end{solution}

\subsection{Rationals}

\textbf{Lemma 4.2.3 (Exercise 4.2.2)} 
We want to show that formal division is well-defined.
As in $a//b = a'//b' \implies a//b + c//d = a'//b' + c//d, a//b-c//d = a'//b'-c//d, a//b \cdot c//d = a'//b' \cdot c//d$

\begin{solution}
\begin{proof}
    1. $a//b=a'//b' \implies (a//b)+(c//d) = (a'//b')+(c//d)$
    \begin{align*}
        a//b = a'//b' &\implies ab' = a'b && \text{4.2.1} \\
        &\implies ab'd^2 = a'bd^2 \\
        &\implies ab'd^2 + bcb'd = a'bd^2 + bcb'd \\
        &\implies adb'd + bcb'd = a'dbd + b'cbd \\
        &\implies (ad+bc)(b'd) = (a'd + b'c)(bd) \\
        &\implies (ad + bc) // bd = (a'd + b'c)//b'd && \text{def 4.2.1}\\
        &\implies (a//b) + (c//d) = (a'//b')+(c//d) && \text{def 4.2.1}
    \end{align*}
    2. 
    \begin{align*}
        a//b = a'//b' &\implies ab' = a'b \\
        &\implies ab'd^2=a'bd^2 \\
        &\implies ab'd^2-bcb'd=a'bd^2-bcb'd \\
        &\implies (ad-bc)(b'd) = (a'd-b'c)(bd) \\
        &\implies (ad-bc)//bd = (a'd-b'c)//(b'd)\\
        &\implies (a//b)-(c//d)=(a'//b')-(c//d)
    \end{align*}
    3.
    \begin{align*}
        a//b=a'//b' &\implies ab'=a'b \\
        &\implies ab'cd=a'bcd \\
        &\implies acb'd=a'cbd \\
        &\implies ac//bd-a'c//b'd \\
        &\implies a//b \cdot c//d = a'//b' \cdot c//d
    \end{align*}
\end{proof}
\end{solution}
\todo{Add more explanations, verify}

\section{The Real Numbers}

\textbf{Lemma 5.3.14} 
Let $x$ be a non-zero real number. Then $x= LIM_{n \to \infty} a_n$ for a Cauchy sequence ${(a_n)}_{n=1}^{\infty}$ which is bounded away from zero.

\begin{solution}
    My solution sketch: we know that $x$ is real so it is the limit of some Cauchy sequence ${(b_n)}_{n=1}^{\infty}$.
    If we can show that for some $N$, eventually $b_n$ is bounded away from $0$ for all $n > N$, we are done, because we can just construct a Cauchy sequence beginning at $N$.
    My approach to doing this was to show that for any non-zero real $x = LIM_{n \to \infty}a_n$, we would be able to find a rational $y$ such that for some $N$, $n>N$ implies $|a_n|>y$.
    With this, we could find that $N$ and construct the new Cauchy sequence starting at ${(a_n)}_{n=N}^{\infty}$.
    It would be straightforward to also show that this real number implied by the Cauchy sequence is equivalent to $x$.

    This solution is a bit tortuous and requires proving a number of smaller lemmas (showing that for some non-zero real the Cauchy sequence will eventually have a non-zero lower bound, subsequences of Cauchy sequences are still Cauchy, etc).
    Tao's solution appear to be much more direct:
    we know that there is some Cauchy sequence for $x$ that is not necessarily bounded away from zero.
    Yet since $x \neq 0$, ${(b_n)}_{n=1}^{\infty}$ is not equivalent to ${(0)}_{n=1}^{\infty}$.
    So it is not eventually $\epsilon$ close for some $\epsilon$.
    Using the triangle inequality, he was then able to show that after a certain point, $|b_n|> \frac{\epsilon}{2}$ for all $n>N$.

    Notably, what he did differently was use the fact that $x \neq 0$ to find some $\epsilon$ such that $x, 0$ were never $\epsilon$ close, which allowed him to avoid some of the lemmas that I had to prove.
\end{solution}

\textbf{Lemma 5.3.15} 
If ${(a_n)}_{n=1}^{\infty}$ is a Cauchy sequence that is bounded away from 0, then ${(a_n^{-1})_{n=1}^{\infty}}$ is also Cauchy and bounded away from 0.

\begin{solution}
\begin{proof}
    From lemma 5.3.14 that since $a_n$ is bounded away from 0, we know that there is some $M>0$ such that $|a_n| > M$ for all $n$.
    If ${(a_n)}_{n=1}^{\infty}$ is Cauchy, then we have that 
    \[
        \forall \epsilon >0, \exists N \suchthat \forall j,k > N, |a_j - a_k| < \epsilon
    \]
    Then, 
    \begin{align*}
        |a_j^{-1} - a_k^{-1}| &= \left | \frac{a_k-a_j}{a_j a_k}\right | \\
        &= \frac{\left | a_j - a_k \right |}{\left | a_j \right | \left | a_k \right |} \\
        &< \frac{\epsilon}{M^2}
    \end{align*}
    So we can just take that for all $\epsilon>0$, we just need to find a $N$ such that $|a_j-a_k| < \epsilon M^2$, which we can find since $(a_n)$ is Cauchy.
\end{proof}
\end{solution}

\section{Limits of sequences}

\subsection{Convergence and limit laws}
There were a few key takeaways from this section.
First, that formal limits of rationals were equivalent to the standard limits of rationals.
Next, this section defined and proved the limit laws.
Third, the introduction of bounded sequences.
Fourth, the introduction of Cauchy sequences and showing that convergent sequences are Cauchy.

The first 2 are important for developing a theory of limits. 
The third is an important notion in the future because bounded sequence turn up a lot: there are a lot of nice properties that hold if sequences are bounded in different ways.
The last is important because the notion of Cauchy sequences is critical when discussing reals versus rationals.
Specifically, Cauchy sequences are convergent in the real number system, which is not true with the rational number system.
(It is easy to construct a sequence that tends to $\sqrt{2}$ which cannot converge in the rationals.)

\subsection{The extended real number system}
The purpose of this section is to introduce the extended real number system which is the standard real number system but including $\infty, -\infty$ as elements.
The motivation is because often times we see sequences that should converge to `$\infty$' or `$-\infty$', (such as $1, 2, 3, \cdots$, $-1, -2, -3, \cdots$ respectively).
(This is in contrast to a sequence like $1, -2, 3, -4$ which wouldn't converge even in this number system).

The remainder of this section defines operations on and properties in this number system (negation, ordering, supremum, least upper bound/ greatest lower bound).

\subsection{Suprema and infima of sequences}
This section defines suprema and infima of sequences.
The suprema of a sequence is the smallest real number that is an upper bound on all of the elements of the sequence (the least upper bound of the set containing the elements of the sequence).
This crucially hinges on the definition of suprema on sets.

Following this, we can then say something interestng about monotone bounded functions: they must converge (prop 6.3.8).
This is then useful to prove some types of limits: namely $\lim_{n \to \infty}x^n = 0$.

\subsection{Limsup, liminf, and limit points}
This section fleshes out limits in much greater detail by introducing limit points, limit superiors, limit inferiors, etc.
In general, this section deals with situations where a sequence may not seem to converge to a limit, but may have quasi-limit like behaviors in certain places.
This section also introduces useful tests to determine limits such as the comparison principle and the squeeze test.

The most important theorem in this section is likely the completeness of the reals, which demonstrates that any Cauchy sequence of reals must converge.
This is not trivial: this property does not hold for rationals and is one of the crucial differences between the rationals and the reals.

\subsection{Some standard limits}
The main takeaways of this section are the limits of some standard functions.

\subsection{Subsequences}
This section introduces subsequences and some interesting ways they relate with their base sequence.
There are 3 main properties of interest.
The first is that a sequence converges to some value $L$ if every subsequence also converges to $L$ (prop 6.6.5).
The second is that if a sequence has some limit point, there exists a subsequence that converges to that point (prop 6.6.6).
The third is the Bolzano-Weierstrass theorem: every bounded sequence has a convergent subsequence.
(This appears to be a specific instance of Heine-Borel).

There appears to be a notion of duality later on when we discuss continuity of functions with there being two equivalent formulations of continuity: one using a $\epsilon-\delta$ definition of continuity and the other using a sequential definition of continuity.
This section seems to be laying the groundwork for some of these future ideas, by relating sequences and subsequences with limit points.

\subsection{Real exponentiation 2}
We continue defining exponentiation for reals.

\section{Series}
The goal of this section is to develop a coherent theory of the limit of sequences.

\subsection{Finite series}
We start by discussing series of finite sequences and finite sets.
This section seems a bit tedious, but it's important to note that many of these theorems and lemmas do not hold for countably infinite or uncountable sums!
So it is crucial to be as explicit as possible with all of the assumptions and conditions needed.

We define a series as the following:
\[
    \begin{aligned}
        \sum_{i=m}^{n+1} a_i &= \sum_{i=m}^{n} a_i + a_{n+1}\\
        \sum_{i=m}^{n} a_i &= 0: n < m
    \end{aligned}
\]
As a meta note, it seems like whenever it is a little bit tricky to define some mathematical `object' without a `$\dots$' symbol, it might be fairly straightforward to define the concept in terms of induction.
(Another example of this: defining natural numbers as $0, 0 \increment, (0 \increment) \increment, \dots$ vs using induction). 

Following this, we can then prove a few basic lemmas about properties of series (lemma 7.1.4).
Using this definition of series of finite sequences, we can then define a summation over a finite set $E$ by defining a bijective mapping $g^{-1}$ for each element $x \in E$ to an integer $i$, with $g(i) = x$.
This is followed by proving a few propositions and lemmas about finite summations of sets (well-definedness: proposition 7.1.8, summation laws: 7.1.11, summation of cartesian products: lemma 7.1.13, fubini's theorem for finite series: corollary 7.1.14). 

\textbf{Lemma 7.1.4} 
Some properties about summations:

\begin{solution}
    Most of these properties can be proven using induction on the number of elements in the summation.

    (f):
\begin{proof}
    Induction on $n \geq m$.

    Base case: $n=m$. Then we have:
    \[
        \begin{aligned}
            \sum_{i=m}^{n} a_i &= \sum_{i=m}^{n-1}a_i + a_n && \text{def 7.1.1} \\
            &= 0 + a_n && \text{(def 7.1.1) since } n-1 < m \\
            &= a_n \\
            &\leq b_n && \text{(assumption)} \\
            &= 0 + b_n \\
            &= \sum_{i=m}^{n-1}b_i + b_n && \text{(def 7.1.1)} \\
            &= \sum_{i=m}^{n} b_i && \text{(def 7.1.1)}
        \end{aligned}
    \]

    Inductive step:
    Assume $\sum_{i=m}^{n} a_i \leq \sum_{i=m}^{n} b_i$.
    Then:
    \[
        \begin{aligned}
            \sum_{i=m}^{n+1} a_i &= \sum_{i=m}^{n} a_i + a_{n+1} && \text{(def 7.1.1)} \\
            &\leq \sum_{i=m}^n b_i + a_{n+1} && \text{(inductive hyp)} \\
            &\leq \sum_{i=m}^n b_i + b_{n+1} && \text{(assumption)} \\
            &= \sum_{i=m}^{n+1} b_i && \text{(def 7.1.1)}
        \end{aligned}
    \]
    And we are done.
\end{proof}
\end{solution}

We define summations on set $M$ by giving each element $a \in M$ a value $f(a)$.
To compute the summation over the set, we then have to somehow define a bijection $g: [n] = \{1 \leq i \leq n\} \to M$.
Then $\sum_{x \in X}f(x) = \sum_{i=1}^{n} f(g(i))$.
Property 7.1.8 shows that this is well-defined for the finite case.


\textbf{Lemma 7.1.11} Basic properties of summation over finite sets:


\begin{solution}

    (a): If $X$ is empty and $f: X \to \mathbf{R}$ is a function, we have $\sum_{x \in X}f(x) = 0$.
\begin{proof}
    \[
        \begin{aligned}
            \sum_{x \in X} f(x) &= \sum_{i=1}^{0} f(g(i)) && \text{def 7.1.6} \\
            &= 0 && \text{def 7.1.1}
        \end{aligned}
    \]
\end{proof}
(b): If $X$ is contains a single element $x_0$ and $f: X \to \mathbf{R}$ is a function, we have $\sum_{x \in X}f(x) = f(x_0)$.
\begin{proof}
    \[
        \begin{aligned}
            \sum_{x \in X} f(x) &= \sum_{i=1}^{1} f(g(i)) && \text{def 7.1.6} \\
            &= f(g(1)) && \text{def 7.1.1}
            &= f(x_0) && \text{def of $g: \{1\} \to X$}
        \end{aligned}
    \]
\end{proof}
(i): Let $X$ be a finite set and let $f: X \to \mathbf{R}$ be a function.
Then: $|\sum_{x \in X} f(x)| \leq \sum_{x \in X}|f(x)|$
\begin{proof}
    \[
        \begin{aligned}
            |\sum_{x \in X} f(x)| &= |\sum_{i = 1}^{n}f(g(i))| && \text{def 7.1.6}\\ 
            &\leq \sum_{i=1}^{n}|f(g(i))| && \text{prop 7.1.4e} \\
            &= \sum_{x \in X} |f(x)| && \text{def 7.1.6}
        \end{aligned}
    \]
\end{proof}
\end{solution}

\textbf{Lemma 7.1.13} Let $X,Y$ be finite sets with $f: X \times Y \to \mathbf{R}$ as a function.
Then $\sum_{x \in X}(\sum_{y \in Y} f(x,y)) = \sum_{(x,y) \in X \times Y} f(x,y)$.

Importantly, something about this proof must fail for infinite sets.
(Apparently later on we will prove a similar lemma for infinite sets that only holds under certain convergence conditions.)

When solving this, I neglected to think about using a proof by induction:
I should have instantly considered that since the size of sets are natural numbers and that many of the previous proofs also used induction.

\begin{solution}
\begin{proof}
    Induction on the size of set $Y$: $n_y$

    Base case: $n_y=0$.
    Then:
    \[
        \begin{aligned}
            \sum_{x \in X}(\sum_{y \in Y}) &= \sum_{x \in X}0 && \text{(7.1.11a)} \\
            &= \sum_{i=1}^{n_x} 0 && \text{def 7.1.6, $|X| = n_x$} \\ 
            &= 0 \\
        \end{aligned}
    \]
    Note that if $Y = \emptyset, X \times Y = \emptyset$, so $\sum_{(x,y) \in X \times Y} f(x,y) = 0 = \sum_{x \in X}(\sum_{y \in Y} f(x,y))$

    Inductive step:
    if this holds for $n_y=k$, consider $n_y=k+1$:
    \[
        \begin{aligned}
            \sum_{x \in X}(\sum_{y \in Y} f(x,y)) &= \sum_{x \in X}(\sum_{y \in Y'}f(x,y) + f(x,y_0)) &&  (Y' = Y/{y_0}) \\
            &= \sum_{x \in X}(\sum_{i=1}^k f(x, h(i)) + f(x, h(k+1))) && \text{lemma 7.1.6}\\
            &= \sum_{j=1}^{n_x}(\sum_{i=1}^k f(h_x(j), h(i)) + f(h_x(j), h(k+1))) && \text{lemma 7.1.6}\\
            &= \sum_{j=1}^{n_x}(\sum_{i=1}^k f(x, h(i))) + \sum_{j=1}^{n_x}f(x, h(k+1)) && \text{lemma 7.1.4c}\\
            &=\sum_{x \in X} \sum_{y \in Y'} f(x,y) + \sum_{x \in X} f(x,y_0) && \text{lemma 7.1.6} \\
            &= \sum_{(x,y) \in X \times Y'} f(x,y) \sum_{x \in X} f(x, y_0) && \text{inductive step} \\
            &= \sum_{(x,y) \in X \times Y} f(x,y) + \sum_{(x,y)\in X \times \{y_0\}f(x,y)} && \text{strong induction? 7.1.11c?} \\
            &= \sum_{(x,y) \in X \times Y} f(x,y) && \text{7.1.11e}
        \end{aligned}
    \]
    And we are done.
\end{proof}
\end{solution}

\subsection{Infinite series}
We now introduce summations of countably infinite terms.
We formalize this by taking the limit of a finite series.
Essentially this means that $\sum_{n=1}^{\infty} a_n = \lim_{N \to \infty} \sum_{n=1}^{N}$.
By being explicit about how we formalize infinite sums, it becomes clear that an infinite summation is only meaningful if it converges as $N \to \infty$.

The remainder of this section discusses conditions to determine if a series will converge or diverge (zero test, absolute convergence test, alternating series test, telescoping series) as well as generally useful lemmas/propositions (series laws).

It's in this section where we're starting to see the full power of what analysis has to offer, as many of these conditions may not be obvious without defining finite and infinite series formally.

\subsection{Sums of non-negative numbers}
If we restrict summations to non-negative numbers, we can find a few interesting properties.
Note that the summations will be monotone, so if they're bounded, they will converge (this is prop 7.3.1).
A few other useful properties and tests of convergence come up: the comparison test, Cauchy criterion.
The Cauchy criterion is particularly interesting: given only a fraction of the elements in a sequence, we can determine if the whole series is convergent or not as well as give rough bounds to what value it converges to.

Using these new properties, we are able to say more about series like geometric series, harmonic series about when they converge/diverge.
These are fundamental series that show up often in math.

\subsection{Rearrangement of Series}
The goal of this section is to develop a theory of when we can rearrange infinite series and still get the same value.
It turns out that there are 2 useful lemmas regarding this.

\textbf{Lemma 7.4.1}
Let $\sum_{n=0}^{\infty} a_n$ be a convergent series of non-negative real numbers and $f: \mathbf{N} \to \mathbf{N}$ is a bijection.
Then $\sum_{n=0}^{\infty} a_{f(n)}$ is convergent and has the same sum:
\[
    \sum_{n=0}^{\infty} a_n = \sum_{m=0}^{\infty} a_{f(n)}
\]

Note: something about this proof must be conditional on $a_n$ being non-negative.
This is because $\sum_{n=1}^{\infty} \frac{{(-1)}^{n+1}}{n}$ converges by the alternating series test yet can be rearranged to get different values.

\begin{solution}
    Slightly sketchy proof, but it can be made a bit more formal.
\begin{proof}
    If $\sum_{n=0}^{\infty} a_n$ is convergent, then let's say that $\sum_{n=0}^{\infty} a_n = \lim_{N \to \infty} \sum_{n=0}^{N} a_n = L$.
    Note that for any $M$ such that $\sum_{m=0}^{M} a_{f(m)}$, there is an $N$ such that $\sum_{n=0}^{N}a_n \geq \sum_{m=0}^{M} a_{f(m)}$.
    (This can be shown using induction probably while using the fact that $a_n$ must be non-negative).

    But this means that $\sum_{m=0}^{M} a_{f(n)}$ is bounded by $L$ from above for all values $M$, and it is bounded by 0 from below since all values are non-negative.
    Therefore by proposition 6.3.8 (monotone bounded sequences converge), we have that $\sum_{m=0}^{\infty}a_{f(m)}$ converges.

    Essentially, we now have that $\sum_{m=0}^{\infty} a_{f(m)} \leq \sum_{n=0}^{\infty} a_n$.
    The reverse direction should probably be similar to prove (maybe using the fact that $f$ is a bijection).
\end{proof}
\end{solution}

\textbf{Lemma 7.4.3}
Let $\sum_{n=0}^{\infty} a_n$ be an absolutely convergent series of real numbers and $f: \mathbf{N} \to \mathbf{N}$ is a bijection.
Then $\sum_{n=0}^{\infty} a_{f(n)}$ is absolutely convergent and has the same sum:
\[
    \sum_{n=0}^{\infty} a_n = \sum_{m=0}^{\infty} a_{f(n)}
\]

As before, something about this proof requires the absolute convergent condition.
This is because $\sum_{n=1}^{\infty} \frac{{(-1)}^{n+1}}{n}$ is conditionally convergent yet the sum can be rearranged to get different values.

\begin{solution}
    This proof is slightly complicated.
    (It took multiple readings for me).
    \begin{proof}
        It's fairly simple to show that $\sum_{n=0}^{\infty} a_{f(n)}$ must be absolutely convergent.
        Absolute convergence means that $\sum_{n=0}^{\infty} |a_n|$ converges.
        We have by prop 7.4.1 that then $\sum_{m=0}^{\infty} |a_{f(m)}|$ must converge.
        (This is because each term is non-negative).
        Therefore $\sum_{n=0}^{\infty} a_{f(n)}$ is absolutely convergent.

        The difficult part is showing that the two series have the same sum.
        Let's say that $\sum_{n=1}^{\infty} a_n = L$.
        We know that $\sum_{n=1}^{\infty} |a_n|$ converges.
        Our end goal is the following: we want to show that for all $\epsilon > 0$ there exists  $M$ such that for all $M' > M: |\sum_{m=1}^{M'} a_{f(m)} -L| < \epsilon$ (basically that the sum converges to $L$).

        The key insight is that $\sum_{m=1}^{M'} a_{f(m)}$ can be made arbitrarily close to $\sum_{n=1}^{N'} a_n$, which is arbitrarily close to $L$.

        We know that for all $\epsilon >0$, there exists $N_1$ such that for all $N'> N_1$, 
        \[
            \left | \sum_{n=1}^{N'}a_n - L \right | < \epsilon
        \]
        This comes from the definition of convergence of the series $\sum_{n=1}^{\infty} a_n$.
        Note that this means that we can take any $\frac{\epsilon}{2}$ and find $N_1$ such that for all $N' \geq N$, 
        \begin{align*}
        \left | \sum_{n=1}^{N'} a_n - L \right |  < \frac{\epsilon}{2} && (1)
        \end{align*}

        We also need to pick $N_2$ such that for all $j,k > N_2$, 
        \begin{align*}
            \left | \sum_{n=j}^k a_n \right | < \frac{\epsilon}{2} && (2)
        \end{align*}

        Note that this is only possible because $\sum_{n=1}^{\infty} |a_n|$ is convergent and from prop 7.2.5.
        Now simply take $N = \max(N_1, N_2)$.
        (What we essentially just did is pick a value for $N$ such that both (1) and (2) are simultaneously satisfied.)

        Take $M = \max(f^{-1}(n))_{n=1}^{N}$.
        Note that 
        \[
            \sum_{n=1}^{M} a_{f(m)} = \sum_{n=1}^{N} a_n + \sum_{x \in X} a_x
        \]
        where 
        \[
            X = \{ f(m): 1 \leq n \leq N\} / \{ n: 1 \leq n \leq N\}
        \]

        Another key insight here:
        every term in $\sum_{n=1}^{N}a_n$ should also be in $\sum_{m=1}^{M} a_{f_m}$.
        (Indeed, we pick $M$ specifically such that this condition is satisfied.)

        So now:
        \begin{align*}
            \left | \sum_{m=1}^{M} a_{f(m)} - L \right | &= \left | \sum_{n=1}^{N} a_n + \sum_{x \in X} a_x - L \right | \\
            &\leq \left | \sum_{n=1}^{N} a_n -L \right | + \left | \sum_{x \in X} a_x \right | && \text{(triangle inequality)}\\
            &\leq \frac{\epsilon}{2} + \left | \sum_{x \in X} a_x \right | && \text{(1) from above} \\
            &\leq \frac{\epsilon}{2} + \left | \sum_{n=N+1}^{M} a_n \right | && \text{(3)}\\
            &\leq \frac{\epsilon}{2} + \sum_{n=N+1}^{M} \left | a_n \right | && \text{(absolute convergence test)}\\
            &\leq \frac{\epsilon}{2} + \frac{\epsilon}{2} && \text{(2) from above} \\
            &= \epsilon
        \end{align*}
        where (3) holds because $X \subseteq \{m: 1 \leq m \leq M \}$.

        Note that we're done.
    \end{proof}
    The reason this proof fails for conditional convergence is because we cannot bound $\sum_{N+1}^{M} |a_n|$ by $\frac{\epsilon}{2}$ if $\sum_{n=1}^{\infty} |a_n|$ does not converge.
    For example, consider a conditionally convergent series $\sum_{n=1}^{\infty} \frac{1}{n}$.
    $\frac{1}{N+1} + \frac{1}{N+2} + \cdots + \frac{1}{M}$ can be arbitrarily large which means that we can't necessarily bound the expression by some $\frac{\epsilon}{2}$.
\end{solution}

\subsection{The Root and Ratio Tests}
This section concluded by proving the root and ratio tests, which are further useful for determining additional limits.
For example, we're now able to use this to determine $\lim_{n \to \infty} n^{1/n}$.

\section{}

\section{}

\section{Differentiation of Functions}
\subsection{Basic definitions}
This section introduced the formal definition of a derivative.
The canonical counterexample to separate continuous functions from differentiable functions is $f(x) = |x|$ which is the former but not the latter.
Following this, a number of useful properties are demonstrated: differentiability implies continuity, differential function laws, chain rule.

\subsection{Local maxima, local minima, and derivatives}
This section establishes the connection between local minima, local maxima and having a derivative of 0.
(This is prop 10.2.6)
This section also introduces Rolle's theorem and the intermediate value theorem, which are both relevant for derivatives and local minima/maxima.

\subsection{Monotone functions and derivatives}
There is a relationship between the derivative of a function and whether it is monotonically increasing or decreasing.
However, it's more subtle than simply `$\frac{d}{dx}f(x)\geq 0$ if an only if $f$ is monotonically increasing' (for example).
Prop 10.3.1 shows that if $f$ is monotonic increasing and is differentiable at $x_0$, then $\frac{d}{dx}f(x) \geq 0$.
(This is subtle and interesting because there are certainly monotonic increasing functions that are continuous yet do not have $\frac{d}{dx}f(x) \geq 0$).
We then show something related about strictly monotonic functions.

\subsection{Inverse functions and derivatives}
We prove the inverse function theorem.
It's apparently useful for proving that $\frac{d}{dx} x^c = cx^{c-1}$ when $c$ is not necessarily an integer or rational.

\subsection{L'H\^opital's Rule}
We prove L'H\^opital's Rule.
This is a useful rule to find limits that comes up a few times in an intro calculus class (such as for dealing with limits like $\lim_{x \to 0} \frac{\sin(x)}{x}$).

\section{The Riemann Integral}
The goal of this chapter is to work our way up to defining the Riemann integral.

\subsection{Partitions}
The first section discusses partitions and how one can partition a large interval into smaller intervals.
We define an introductory notion of connectedness, prove that bounded intervals are equivalent to bounded and connected sets, and show that the length of an interval must be equal to the sum of the lengths of the intervals in the partition.
All of these are pretty intuitive and proving them isn't particularly difficult, but it's necessary to make things rigorous.

\subsection{Piecewise constant functions}
This section introduces piecewise constant functions as a particularly easy type of function to integrate.
The basic idea here is that if you have a function $f$ defined on some interval $I$, and there is some partition $P$ of that interval into smaller intervals $J \in P$ such that $f$ is constant on each of those smaller intervals, then the integral will basically be $\int_{I} f = \int_{P} f = \int_{J \in P} c_J |J|$. (Definition 11.2.9)

To ensure that this is well-defined, we have to ensure that this value is equal regardless of which partition is chosen.
(Prop 11.2.13)
(As well as dealing with edge cases like if the interval is empty or a single point, both of which turn out to have integral 0).

This section concludes with the laws of integration over piecewise constant functions.

\subsection{Upper and Lower Riemann Integrals}
This section takes integration of piecewise functions and extends it to general Riemann integrals by using the notions of supremum and infimum.
This notion of `taking a simple construction and taking the supremum/infimum' seems to come up often: elementary sets to Jordan sets.
We define upper and lower Riemann integrals using the supremum and infimum of piecewise integrals respectively.
If the two are equal, we say that this is Riemann integrable.
We later define Riemann sums.

\subsection{Basic properties of Riemann Integral}
This section shows many basic properties of Riemann integration.
Some key ones were that if $f,g$ are Riemann integrable, then $\int \max(f,g)$ and $\int fg$ are Riemann integrable.

\subsection{Riemann integrability of Continuous functions}
On bounded intervals, uniformly continuous functions are Riemann integrable.
On bounded intervals, continuous and bounded functions are Riemann integrable.

\subsection{Riemann Integrability of Monotone Functions}
This was surprising to me.
It turns out that monotone functions are Riemann integrable under specific conditions (domain is closed or function is bounded).
Note that the function does not need to be continuous

\subsection{Non-Riemann Integrable Function}
It's $f(x) = \begin{cases} 0: x \in \mathbf{Q} \\ 1: x \in \mathbf{R}/\mathbf{Q} \end{cases}$.

\subsection{Riemann-Stieltjes Integral}
When constructing a piecewise RS integral, we do something slightly different.
Instead of $p.c. \int_{[P]} f:= \sum_{J \in \mathbf{P}} c_J|J|$ as with Riemann integrals, we do $p.c. \int_{[P]} f\, d\alpha := \sum_{J \in \mathbf{P}} c_J \alpha(J)$.
Note that the former is a special case of the latter when $\alpha(J) = |J|$.
It's unclear exactly what this generalization is useful for, but we're able to demonstrate many of the previous theorems of Riemann integrals with these Riemann-Stieltjes integrals.

\subsection{The Two Fundamental Theorems of Calculus}
These two theorems connect integration and differentiation (the integral of the derivative and the derivative of the integral).

The first theorem states that if $a < b$, $f:[a,b] \to \mathbf{R}$ is a Riemann integrable function, and $F:[a,b] \to \mathbf{R}$, is the function $F(x) = \int_{[a,b]} f$, then $F$ is continuous.
Additionally, if $x_0 \in [a,b]$ and $f$ is continuous at $x_0$, then $F$ is differentiable at $x_0$ and $F'(x_0) = f(x_0)$.
Informally, this is saying that $(\int_{[a,b]}f)'(x) = f(x)$.

The second theorem states that if $a \leq b$, $f: [a,b] \to \mathbf{R}$ is Riemann integrable, and $F: [a,b] \to \mathbf{R}$ is antiderivative of $f$, then $\int_{[a,b]}f = F(b)-F(a)$.

\subsection{Consequences of the Fundamental Theorems}
The chapter concludes by proving several commonly used properties in calculus: the integration by parts and change of variables formulas.

\section{Resources}
Terence Tao: \href{https://terrytao.wordpress.com/books/analysis-i/}{Analysis 1}


\end{document}
