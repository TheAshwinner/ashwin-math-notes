\documentclass[answers,12pt]{exam}
\usepackage[utf8]{inputenc, xcolor}
\usepackage{hyperref}
\usepackage{amsmath,amsthm,amssymb,empheq}
\usepackage{float}
\usepackage{todonotes}

\newcommand{\increment}{\text{++}}
\newcommand{\fakeminus}{\text{ --- }}
\newcommand{\suchthat}{\text{ s.t. }}

\title{Analysis Solutions}
\author{Ashwin Sreevatsa}
\date{April 2023}

\begin{document}

\maketitle

\section*{Quick Remarks}
This will be a solution page for problems in Terence Tao's Analysis 1 (Fourth edition).
The problems I'm adding here are those that I worked on and solved myself.
I don't expect to add all the problems in the textbook, but rather those that I find the most interesting.
I will try and do a few problems from each section though, just to make sure I'm actually understanding the concepts properly.

\section{Introduction}

\section{Starting at the Beginning: The Natural Numbers}

\textbf{Proposition 2.1.6:} 4 is not equal to 0 
\todo{Add column proofs.}

\begin{solution}
    \[
        \begin{aligned} 
            &4 = 3\increment \text{ (definition 2.1.3)} \\
            &3\increment \text{ is a natural number (axiom 2.1, axiom 2.2).} \\
            &3\increment \neq 0 \text{ (axiom 2.3)}
        \end{aligned}
    \]
\end{solution}

\textbf{Proposition 2.1.8:} 6 is not equal to 2
\begin{solution}
    \[
        \begin{aligned}
            &6=2\\
            \implies &5 \increment = 1 \increment \text{ (axiom 2.2, def 2.1.3)}\\
            \implies &5 = 1 \text{ (axiom 2.4)}\\
            \implies &4 \increment = 0 \increment \text{ (axiom 2.2, def 2.1.3)}\\
            \implies &4 = 0 \text{ (axiom 2.4)}\\
        \end{aligned}
    \]
    We know that this is false from prop 2.1, so $6 \neq 2$.
\end{solution}

\textbf{Proposition 2.1.16:}  

\begin{solution}

\end{solution}

\textbf{Lemma 2.2.2:}  $n+0=n$

\begin{solution}
We use induction.
Let property $P(n)$ be that $n+0=n$.
$P(0): 0+0=0$, which holds by the definition of addition (1).
We want to show that $P(n) \implies P(n \increment)$.
If $n+0=n$ by the inductive hypothesis, then 
\begin{align*}
    (n \increment) + 0 &= (n+0) \increment && \text{def of addition 2}\\
    &= n \increment && \text{inductive hypothesis}
\end{align*}
So $P(n)$ holds for all natural numbers $n$.
\end{solution}

\textbf{Lemma 2.2.3:}  $n+ (m \increment) = (n+m) \increment$

\begin{solution}
Let $P(n): n+ (m \increment) = (n+m) \increment$.
Base case: 
\begin{align*}
    P(0): 0+ (m \increment) &= m \increment && (1) \\
    &= (0 + m) \increment && (1)
\end{align*}
Inductive step: $P(n) \implies P(n \increment)$
\begin{align*}
    (n \increment) + (m \increment) &= (n+ (m \increment)) \increment && (2) \\
    &= ((n+m) \increment) \increment && \text{inductive step} \\
    &= ((n \increment) + m) \increment && (2) \\
\end{align*}
By axiom 2.5, $P(n)$ holds for all natural numbers.
\end{solution}

\textbf{Corollary 2.2.3.1} (Technically not an actual problem, just a note): $n \increment = n+1$

\begin{solution}
    \begin{align*}
        n \increment &= (n+0) \increment && \text{Lemma 2.2.2} \\
        &= n + (0 \increment) && \text{Lemma 2.2.3}\\
        &= n+1 && \text{Definition of 1}
    \end{align*}
\end{solution}

\textbf{Prop 2.2.4} Commutativity of natural numbers in addition: $P(n): n+m = m+n$

\begin{solution}
    We use induction to show property $P(n): n + m = m + n$

    Base case $P(0)$
    \begin{align*}
        0+m &= m && (1) \\
        &= m+0 && \text{(lemma 2.2.2)}
    \end{align*}
    Inductive step $P(n) \implies P(n\increment)$:
    \begin{align*}
        (n \increment) + m &= (n+m) \increment && (2) \\
        &= (m+n) \increment && \text{(inductive hyp)} \\
        &= m+ (n \increment) && \text{(lemma 2.2.3)}
    \end{align*}
    By axiom 2.5, $P(n)$ holds for all natural numbers.
\end{solution}

\textbf{Prop 2.2.5} Commutativity of addition over natural numbers

\begin{solution}
    Let $P(c):= (a+b)+c = a+(b+c)$, with $a,b,c$ as natual numbers.
    We use induction to prove this.
    
    Base case: $P(0)$:
    \begin{align*}
        (a+b)+0 &= a+b && \text{lemma 2.2.2}\\
        &= a+(b+0) && \text{lemma 2.2.2}
    \end{align*}

    Inductive hypothesis: $P(n) \implies P(n \increment)$
    \begin{align*}
        (a+b)+ c \increment &= ((a+b)+c) \increment && \text{lemma 2.2.3} \\
        &=(a+(b+c))\increment && \text{inductive hypothesis} \\
        &= a + (b+c) \increment && \text{lemma 2.2.3} \\
        &= a+(b+c \increment) && \text{lemma 2.2.3}
    \end{align*}
    By axiom 2.5 (or whatever), this holds for all natural numbers and we are done.
\end{solution}

\textbf{Prop 2.2.6} Additive cancellation law

\begin{solution}
    Let's say $P(a): a+b = a+c \implies b=c$.
    We want to show that this property holds via induction.

    Base case $P(0)$: we have $0+b=0+c$.
    Then:
    \begin{align*}
        b &= 0+b && (1) \\
        &= 0+c && \text{assumption} \\ 
        &= c && (1)
    \end{align*}

    Inductive step $P(n) \implies P(n \increment)$.

    We have that $a+b=a+c \implies b=c$ and $(a \increment) +b  = (a \increment) + c$.
    \begin{align*}
        (a + b) \increment &= (a \increment) + b && (2) \\
        &= (a \increment) + c && \text{assumption} \\ 
        &= (a+c) \increment && (2)
    \end{align*}
    We then have $a+b=a+c$ from axiom 2.4 $(a \increment = b \increment \implies a=b)$.

    By axiom 2.5, this holds for all natural numbers.
\end{solution}

\section{Set Theory}

\section{Integers and Rationals}
\subsection{4.1 Integers}
\textbf{Lemma 4.1.3} Addition and multiplication are well-defined.
Let $a,b,a',b',c,d$ be natural numbers.
If $(a \fakeminus b)=(a' \fakeminus b')$, then $(a \fakeminus b)+(c \fakeminus d)=(a' \fakeminus b')+(c \fakeminus d)$ and $(a \fakeminus b)\times(c \fakeminus d)=(a' \fakeminus b')\times(c \fakeminus d)$.
(Note that \fakeminus{} is not the same as subtraction since subtraction hasn't yet been defined.)

\begin{solution}
\begin{proof}
    To show the first part:
    \begin{align*}
        (a \fakeminus b) + (c \fakeminus d) &= (a' \fakeminus b') + (c \fakeminus d)\\
        \iff (a+c) \fakeminus (b+d) &= (a' + c) \fakeminus (b' + d) && \text{(Def 4.1.2)}\\
        \iff (a+c) + (b'+d) &= (a' + c) + (b + d)  && \text{(Def 4.1.1)}\\
        \iff (a+b') &= (a'+b)  && \text{(Asso., comm., prop 2.2.6)}\\
        \iff (a \fakeminus b) &= (a' \fakeminus b')  && \text{(Def 4.1.1)}
    \end{align*}
    To show the second part, note that we want $(a \fakeminus b) = (a' \fakeminus b') \implies (a \fakeminus b) \times (c \fakeminus d)  = (a' \fakeminus b') \times (c \fakeminus d)$:
    \begin{align*}
        (a \fakeminus b) \times (c \fakeminus d) &= (a' \fakeminus b') \times (c \fakeminus d) \\
        \iff (ac + bd) \fakeminus (ad + bc) &= (a'c + b'd) \fakeminus (a'd + b'c)&& \text{(Def 4.1.2)} \\
        \iff (ac + bd) + (a'd + b'c) &= (a'c + b'd) + (ad + bc) && \text{(Def 4.1.1)}\\
        \iff (a+b')c + (a'+b)d &= (a'+b)c + (a+b')d && \text{(Asso., comm., dist.)}
    \end{align*}
    But clearly this last statement is true from the assumption that $(a \fakeminus b) = (a' \fakeminus b') \iff (a + b') = (a' + b)$ and substituting the terms as needed.
\end{proof}

Note for posterity: originally I was attempting to prove the second part by showing that $(a \fakeminus b) \times (c \fakeminus d) = (a' \fakeminus b') \times (c \fakeminus d) \iff (a \fakeminus b) = (a' \fakeminus b')$ similar to the first part.
This is a false statement in general (just take $c = d$), yet it didn't occur to me to question it until I got repeatedly stuck.
\end{solution}

\textbf{Lemma 4.1.3} (Laws of algebra for addition)

\begin{solution}
\begin{proof}
    Note that for most of these, we take $x = a \fakeminus b$, where $x$ is an integer and $a,b$ are natural numbers.

    Reflexivity:
    \begin{align*}
        x+y &= (a \fakeminus b) + (c \fakeminus d) \\
        &= (a+c) \fakeminus (b+d) \\
        &= (c+a) \fakeminus (d+b) \\
        &= (c \fakeminus d) + (a \fakeminus b) \\
        &= y+x
    \end{align*}

    Associativity
    \begin{align*}
        (x+y)+z &= ((a \fakeminus b) + (c \fakeminus d)) + (e \fakeminus f) \\
        &= ((a+c) \fakeminus (b+d)) + (e \fakeminus f) \\
        &= ((a+c)+e) \fakeminus ((b+d)+f) \\
        &= (a+(c+e)) \fakeminus (b+(d+f)) \\
        &= (a \fakeminus b) + ((c+e) \fakeminus (d+f)) \\
        &= (a \fakeminus b) + ((c \fakeminus d) + (e \fakeminus f)) \\
        &= x + (y+z)
    \end{align*}

    Additive identity
    \begin{align*}
        x+0 &= (a \fakeminus b) + 0 \\
        &= (a \fakeminus b) + (0 \fakeminus 0) \\
        &= (a+0) \fakeminus (b+0) \\
        &= a \fakeminus b \\
        &= x
    \end{align*}
    \begin{align*}
        x+0 &= (a \fakeminus b) + 0 \\
        &= (a \fakeminus b) + (0 \fakeminus 0) \\
        &= (a+0) \fakeminus (b+0) \\
        &= (0+a) \fakeminus (0+b) \\
        &= 0 + x
    \end{align*}

    Additive inverse
    \begin{align*}
        x + (-x) &= (a \fakeminus b) + (- (a \fakeminus b)) \\
        &= (a \fakeminus b) + (b \fakeminus a) \\
        &= (a+b) \fakeminus (b+a) \\
        &= (a+b) \fakeminus (a+b) \\ 
        &= 0 \\
        (-x)+x &= x + (-x) && \text{commutativity of integers}
    \end{align*}

    Multiplicative commutativity
    \begin{align*}
        xy &= (a \fakeminus b)(c \fakeminus d) \\
        &= (ac + bd) \fakeminus (ad + bc) \\
        &= (ca + db) \fakeminus (da + cb) \\ 
        &= (c \fakeminus d)(a \fakeminus b) \\
        &= yx
    \end{align*}

    Multiplicative associativity
    \begin{align*}
        (xy)z &= ((a \fakeminus b)(c \fakeminus d))(e \fakeminus f) \\
        &= ((ac + bd) \fakeminus (ad + bc))(e \fakeminus f) \\
        &= ((ac + bd)e + (ad + bc)f) \fakeminus ((ad + bc)e + (ac + bd)f) \\ 
        &= (ace + bde + bcf + adf) \fakeminus (bce + ade + acf + bdf) \\
        &= (a \fakeminus b)((ce + df) \fakeminus (cf + de)) \\
        &= (a \fakeminus b)((c \fakeminus d)(e \fakeminus f)) \\
        &= x(yz)
    \end{align*}

    Multiplicative identity
    \begin{align*}
        x \cdot 1 &= (a \fakeminus b) \cdot 1 \\
        &= (a \fakeminus b)(1 \fakeminus 0) \\
        &= (a \cdot 1 + b \cdot 0) \fakeminus (a \cdot 0 + b \cdot 1) \\
        &= (a \fakeminus b) \\
        &= x \\
        x \cdot 1 &= 1 \cdot x
    \end{align*}
    
    Distributive property
    \begin{align*}
        x(y+z) &= (a \fakeminus b)((c \fakeminus d) + (e \fakeminus f)) \\
        &= (a \fakeminus b) ((c+e) \fakeminus (d+f)) \\\
        &= (a(c+e) + b(d+f)) \fakeminus (b(c+e) + a(d+f)) \\
        &= (ac + bd + ae + bf) \fakeminus (bc + be + ad + af) \\
        &= (ac + bd + ae + bf) \fakeminus (be + af + ad + bc) \\
        &= ((ac +bd) \fakeminus (ad + bc)) + ((ae + bf) \fakeminus (be + af)) \\
        &= (a \fakeminus b)(c \fakeminus d) + (a \fakeminus b)(e \fakeminus f) \\
        &= xy + xz
    \end{align*}
\end{proof}
\end{solution}

\subsection{Rationals}

\textbf{Lemma 4.2.3 (Exercise 4.2.2)} 
We want to show that formal division is well-defined.
As in $a//b = a'//b' \implies a//b + c//d = a'//b' + c//d, a//b-c//d = a'//b'-c//d, a//b \cdot c//d = a'//b' \cdot c//d$

\begin{solution}
\begin{proof}
    1. $a//b=a'//b' \implies (a//b)+(c//d) = (a'//b')+(c//d)$
    \begin{align*}
        a//b = a'//b' &\implies ab' = a'b && \text{4.2.1} \\
        &\implies ab'd^2 = a'bd^2 \\
        &\implies ab'd^2 + bcb'd = a'bd^2 + bcb'd \\
        &\implies adb'd + bcb'd = a'dbd + b'cbd \\
        &\implies (ad+bc)(b'd) = (a'd + b'c)(bd) \\
        &\implies (ad + bc) // bd = (a'd + b'c)//b'd && \text{def 4.2.1}\\
        &\implies (a//b) + (c//d) = (a'//b')+(c//d) && \text{def 4.2.1}
    \end{align*}
    2. 
    \begin{align*}
        a//b = a'//b' &\implies ab' = a'b \\
        &\implies ab'd^2=a'bd^2 \\
        &\implies ab'd^2-bcb'd=a'bd^2-bcb'd \\
        &\implies (ad-bc)(b'd) = (a'd-b'c)(bd) \\
        &\implies (ad-bc)//bd = (a'd-b'c)//(b'd)\\
        &\implies (a//b)-(c//d)=(a'//b')-(c//d)
    \end{align*}
    3.
    \begin{align*}
        a//b=a'//b' &\implies ab'=a'b \\
        &\implies ab'cd=a'bcd \\
        &\implies acb'd=a'cbd \\
        &\implies ac//bd-a'c//b'd \\
        &\implies a//b \cdot c//d = a'//b' \cdot c//d
    \end{align*}
\end{proof}
\end{solution}
\todo{Add more explanations, verify}

\section{The Real Numbers}

\textbf{Lemma 5.3.14} 
Let $x$ be a non-zero real number. Then $x= LIM_{n \to \infty} a_n$ for a Cauchy sequence ${(a_n)}_{n=1}^{\infty}$ which is bounded away from zero.

\begin{solution}
    My solution sketch: we know that $x$ is real so it is the limit of some Cauchy sequence ${(b_n)}_{n=1}^{\infty}$.
    If we can show that for some $N$, eventually $b_n$ is bounded away from $0$ for all $n > N$, we are done, because we can just construct a Cauchy sequence beginning at $N$.
    My approach to doing this was to show that for any non-zero real $x = LIM_{n \to \infty}a_n$, we would be able to find a rational $y$ such that for some $N$, $n>N$ implies $|a_n|>y$.
    With this, we could find that $N$ and construct the new Cauchy sequence starting at ${(a_n)}_{n=N}^{\infty}$.
    It would be straightforward to also show that this real number implied by the Cauchy sequence is equivalent to $x$.

    This solution is a bit tortuous and requires proving a number of smaller lemmas (showing that for some non-zero real the Cauchy sequence will eventually have a non-zero lower bound, subsequences of Cauchy sequences are still Cauchy, etc).
    Tao's solution appear to be much more direct:
    we know that there is some Cauchy sequence for $x$ that is not necessarily bounded away from zero.
    Yet since $x \neq 0$, ${(b_n)}_{n=1}^{\infty}$ is not equivalent to ${(0)}_{n=1}^{\infty}$.
    So it is not eventually $\epsilon$ close for some $\epsilon$.
    Using the triangle inequality, he was then able to show that after a certain point, $|b_n|> \frac{\epsilon}{2}$ for all $n>N$.

    Notably, what he did differently was use the fact that $x \neq 0$ to find some $\epsilon$ such that $x, 0$ were never $\epsilon$ close, which allowed him to avoid some of the lemmas that I had to prove.
\end{solution}

\textbf{Lemma 5.3.15} 
If ${(a_n)}_{n=1}^{\infty}$ is a Cauchy sequence that is bounded away from 0, then ${(a_n^{-1})_{n=1}^{\infty}}$ is also Cauchy and bounded away from 0.

\begin{solution}
\begin{proof}
    From lemma 5.3.14 that since $a_n$ is bounded away from 0, we know that there is some $M>0$ such that $|a_n| > M$ for all $n$.
    If ${(a_n)}_{n=1}^{\infty}$ is Cauchy, then we have that 
    \[
        \forall \epsilon >0, \exists N \suchthat \forall j,k > N, |a_j - a_k| < \epsilon
    \]
    Then, 
    \begin{align*}
        |a_j^{-1} - a_k^{-1}| &= \left | \frac{a_k-a_j}{a_j a_k}\right | \\
        &= \frac{\left | a_j - a_k \right |}{\left | a_j \right | \left | a_k \right |} \\
        &< \frac{\epsilon}{M^2}
    \end{align*}
    So we can just take that for all $\epsilon>0$, we just need to find a $N$ such that $|a_j-a_k| < \epsilon M^2$, which we can find since $(a_n)$ is Cauchy.
\end{proof}
\end{solution}

\section{Limits of sequences}

\section{Series}
The goal of this section is to develop a coherent theory of the limit of sequences.
We define a series as the following:
\[
    \begin{aligned}
        \sum_{i=m}^{n+1} a_i &= \sum_{i=m}^{n} a_i + a_{n+1}\\
        \sum_{i=m}^{n} a_i &= 0: n < m
    \end{aligned}
\]
As a meta note, it seems like whenever it is a little bit tricky to define some mathematical `object' without a `$\dots$' symbol, it might be fairly straightforward to define the concept in terms of induction.
(Another example of this: defining natural numbers as $0, 0 \increment, (0 \increment) \increment, \dots$ vs using induction). 

\textbf{Lemma 7.1.4} 
Some properties about summations:

\begin{solution}
    Most of these properties can be proven using induction on the number of elements in the summation.

    (f):
\begin{proof}
    Induction on $n \geq m$.

    Base case: $n=m$. Then we have:
    \[
        \begin{aligned}
            \sum_{i=m}^{n} a_i &= \sum_{i=m}^{n-1}a_i + a_n && \text{def 7.1.1} \\
            &= 0 + a_n && \text{(def 7.1.1) since } n-1 < m \\
            &= a_n \\
            &\leq b_n && \text{(assumption)} \\
            &= 0 + b_n \\
            &= \sum_{i=m}^{n-1}b_i + b_n && \text{(def 7.1.1)} \\
            &= \sum_{i=m}^{n} b_i && \text{(def 7.1.1)}
        \end{aligned}
    \]

    Inductive step:
    Assume $\sum_{i=m}^{n} a_i \leq \sum_{i=m}^{n} b_i$.
    Then:
    \[
        \begin{aligned}
            \sum_{i=m}^{n+1} a_i &= \sum_{i=m}^{n} a_i + a_{n+1} && \text{(def 7.1.1)} \\
            &\leq \sum_{i=m}^n b_i + a_{n+1} && \text{(inductive hyp)} \\
            &\leq \sum_{i=m}^n b_i + b_{n+1} && \text{(assumption)} \\
            &= \sum_{i=m}^{n+1} b_i && \text{(def 7.1.1)}
        \end{aligned}
    \]
    And we are done.
\end{proof}
\end{solution}

We define summations on set $M$ by giving each element $a \in M$ a value $f(a)$.
To compute the summation over the set, we then have to somehow define a bijection $g: [n] = \{1 \leq i \leq n\} \to M$.
Then $\sum_{x \in X}f(x) = \sum_{i=1}^{n} f(g(i))$.
Property 7.1.8 shows that this is well-defined for the finite case.


\textbf{Lemma 7.1.11} Basic properties of summation over finite sets:


\begin{solution}

    (a): If $X$ is empty and $f: X \to \mathbf{R}$ is a function, we have $\sum_{x \in X}f(x) = 0$.
\begin{proof}
    \[
        \begin{aligned}
            \sum_{x \in X} f(x) &= \sum_{i=1}^{0} f(g(i)) && \text{def 7.1.6} \\
            &= 0 && \text{def 7.1.1}
        \end{aligned}
    \]
\end{proof}
(b): If $X$ is contains a single element $x_0$ and $f: X \to \mathbf{R}$ is a function, we have $\sum_{x \in X}f(x) = f(x_0)$.
\begin{proof}
    \[
        \begin{aligned}
            \sum_{x \in X} f(x) &= \sum_{i=1}^{1} f(g(i)) && \text{def 7.1.6} \\
            &= f(g(1)) && \text{def 7.1.1}
            &= f(x_0) && \text{def of $g: \{1\} \to X$}
        \end{aligned}
    \]
\end{proof}
(i): Let $X$ be a finite set and let $f: X \to \mathbf{R}$ be a function.
Then: $|\sum_{x \in X} f(x)| \leq \sum_{x \in X}|f(x)|$
\begin{proof}
    \[
        \begin{aligned}
            |\sum_{x \in X} f(x)| &= |\sum_{i = 1}^{n}f(g(i))| && \text{def 7.1.6}\\ 
            &\leq \sum_{i=1}^{n}|f(g(i))| && \text{prop 7.1.4e} \\
            &= \sum_{x \in X} |f(x)| && \text{def 7.1.6}
        \end{aligned}
    \]
\end{proof}
\end{solution}

\textbf{Lemma 7.1.13} Let $X,Y$ be finite sets with $f: X \times Y \to \mathbf{R}$ as a function.
Then $\sum_{x \in X}(\sum_{y \in Y} f(x,y)) = \sum_{(x,y) \in X \times Y} f(x,y)$.

Importantly, something about this proof must fail for infinite sets.
(Apparently later on we will prove a similar lemma for infinite sets that only holds under certain convergence conditions.)

When solving this, I neglected to think about using a proof by induction:
I should have instantly considered that since the size of sets are natural numbers and that many of the previous proofs also used induction.

\begin{solution}
\begin{proof}
    Induction on the size of set $Y$: $n_y$

    Base case: $n_y=0$.
    Then:
    \[
        \begin{aligned}
            \sum_{x \in X}(\sum_{y \in Y}) &= \sum_{x \in X}0 && \text{(7.1.11a)} \\
            &= \sum_{i=1}^{n_x} 0 && \text{def 7.1.6, $|X| = n_x$} \\ 
            &= 0 \\
        \end{aligned}
    \]
    Note that if $Y = \emptyset, X \times Y = \emptyset$, so $\sum_{(x,y) \in X \times Y} f(x,y) = 0 = \sum_{x \in X}(\sum_{y \in Y} f(x,y))$

    Inductive step:
    if this holds for $n_y=k$, consider $n_y=k+1$:
    \[
        \begin{aligned}
            \sum_{x \in X}(\sum_{y \in Y} f(x,y)) &= \sum_{x \in X}(\sum_{y \in Y'}f(x,y) + f(x,y_0)) &&  (Y' = Y/{y_0}) \\
            &= \sum_{x \in X}(\sum_{i=1}^k f(x, h(i)) + f(x, h(k+1))) && \text{lemma 7.1.6}\\
            &= \sum_{j=1}^{n_x}(\sum_{i=1}^k f(h_x(j), h(i)) + f(h_x(j), h(k+1))) && \text{lemma 7.1.6}\\
            &= \sum_{j=1}^{n_x}(\sum_{i=1}^k f(x, h(i))) + \sum_{j=1}^{n_x}f(x, h(k+1)) && \text{lemma 7.1.4c}\\
            &=\sum_{x \in X} \sum_{y \in Y'} f(x,y) + \sum_{x \in X} f(x,y_0) && \text{lemma 7.1.6} \\
            &= \sum_{(x,y) \in X \times Y'} f(x,y) \sum_{x \in X} f(x, y_0) && \text{inductive step} \\
            &= \sum_{(x,y) \in X \times Y} f(x,y) + \sum_{(x,y)\in X \times \{y_0\}f(x,y)} && \text{strong induction? 7.1.11c?} \\
            &= \sum_{(x,y) \in X \times Y} f(x,y) && \text{7.1.11e}
        \end{aligned}
    \]
    And we are done.
\end{proof}
\end{solution}

\section{Resources}
Terence Tao: \href{https://terrytao.wordpress.com/books/analysis-i/}{Analysis 1}


\end{document}
