\documentclass[answers,12pt]{exam}
\usepackage[utf8]{inputenc, xcolor}
\usepackage{hyperref}
\usepackage{amsmath,empheq,amsfonts,amssymb}
\usepackage{float}
\usepackage{enumitem}
\usepackage[]{mdframed}
\usepackage{todonotes}
\usepackage[normalem]{ulem}

\newcommand{\TODO}{\textcolor{red}{TODO:}}

\title{Neural Tangent Kernels Notes}
\author{Ashwin Sreevatsa}
\date{March 2023}

\begin{document}

\maketitle
\setcounter{section}{-1}

\section*{Quick Remarks}
This doc contains my notes regarding neural tangent kernels, and other assorted deep learning theory-related notes.

\section{Preface}

\section{Neural Tangent Kernels}

\subsection{Soheil Feizi Notes \href{https://www.youtube.com/watch?v=DObobAnELkU}{(link)}}

We're initially considering the following linear regression model: 
\begin{itemize}
    \item Data: ${\{(x_i,y_i)\}}_{i=1}^n, y_i \in \mathbf{R}, x_i \in \mathbf{R}^d$
    \item Function: $f(w,x) = w^{\top} x$. 
    \item Loss function: \[\min \frac{1}{2} \sum_{i=1}^n { (y_i - w^{\top} x_i)}^2\]
    \item Gradient descent update:
    \[ \begin{aligned}
        W(t+1) &= W(t) - \eta_t \nabla L(W_t) \\
        &= W(t) - \eta_t \sum_{i=1}^{n} (f(w,x_i)-y_i) \nabla_w f(w_t, x_i)
        \end{aligned}
    \]
\end{itemize}

Kernel method:

\begin{itemize}
    \item $x \in \mathbf{R}^d \to \phi(x_i) \in \mathbf{R}^D$.
    $D >> d$.
    \item $f(w,x) = w^{\top} \phi(x)$
    \item This is linear in $w$ but not in $x$.
\end{itemize}
The function to be minimized is the following:
\[ 
    \min_w \frac{1}{2} \sum_{i=1}^{n} {(y_i - f(w,x_i))}^2 =
    \min_w \frac{1}{2} \sum_{i=1}^{n} {(y_i - w^{\top}\phi(x_i))}^2
\]
Note that this is also a convex function.

Now, why is this not good enough?
(We have non-linearity and convexity.)
\begin{itemize}
    \item \sout{The function is linear in the weights, meaning it has limited expressivity?
    There's no reason that $f$ is the best model.}
    The non-linearity $\phi$ can in theory be powerful enough to overcome this, I think.
    \item (answers): nonlinearity $\phi$ is fixed.
    How do we know what nonlinearity to use for a problem?
    (\TODO{} Why is this unsatisfying? What would it look like if $\phi$ wasn't fixed? Would you be able to modify the non-linearity for each data point?)
    \item (answers): $D$ might be very large, so it might suffer from curse of dimensionality.
\end{itemize}

The kernel trick basically attempts to circumvent the problem of actually looking at every value in $\phi(x)$ by just considering a closed-form solution for $K(x_i,x_j) = \langle \phi(x_1),\phi(x_2) \rangle$.
(Often this is all we care about, as opposed to the exact values of $\phi(x)$.)
\TODO{} Why is Kernel matrix positive semi-definite?
What does this imply?
\TODO{} What are some examples of the kernel trick (SVMs, ridge regression, PCA, etc)?
\TODO{} Why do we care about the dot product of two different data points?

\begin{mdframed}
Toy example: consider the non-linearity \[\phi({[x_1, \ldots, x_n]}^{\top}) = [x_1^2, \ldots, x_n^2, 2x_1x_2, \ldots, 2x_{n-1}x_n]\]
Then
\[
    \begin{aligned}
    \langle \phi(x), \phi(y) \rangle &=
    \langle {[x_1^2, \ldots, x_n^2, 2x_1x_2, \ldots, 2x_{n-1}x_n]}^{\top}, {[y_1^2, \ldots, y_n^2, 2y_1y_2, \ldots, 2y_{n-1}y_n]}^{\top} \rangle \\
    &= {\left( \sum_{i=1}^n x_i y_i \right)}^2
    \end{aligned}
\]
Note that calculating this inner product explicitly would have taken $O(n^2)$ time and space whereas calculating the kernel takes $O(n)$.
\end{mdframed}

Neural networks:
\begin{itemize}
    \item Gradient with respect to loss function changes
    (This corresponds to the $\nabla_w f(w_t, x_i)$ term which stays fixed in the linear regression case.)
    \item Lazy learning: when the width of a neural network is high, model weights change very slowly.
    \TODO{} why does it depend on $m$ (the width of the neural network)?
    This is an emperical observation.
\end{itemize}
The following is the first order taylor expansion of the neural network around $w_0$.
\[
    f(w,x) \approx f(w_0,x) + \nabla_w {f(w_0,x)}^{\top}(w-w_0)
\]
\TODO{} How is this actually used?
Optimization is convex?
Also why are we talking about this? 

Neural tangent kernel uses the nonlinearity $\phi(x) = \nabla_w f(w_0,x)$.
This means that the kernel is $k(x_1,x_2) = \langle \nabla_w f(w_0,x_1), \nabla_w f(w_0,x_2) \rangle$
Note that this is the linear approximation of the Taylor expansion of the vector function (hence `tangent')

Note: the gradient update does not change for each timestep.
Key clarification is here: $\nabla_w f$ describes the gradient of a function with respect to $w$.
We then evaluate the function at $(w_0,x)$.
This means that the gradient won't change even as the function is proceeding on its trajectory.

We can treat the kernels as the expectations over the unknown distributions $a_i, b_i$ are drawn from.
\TODO{} add notes on how kernel is generated, $\sqrt(m)$ factor.
Neural tangent kernel, infinite width stuff. 
Why do we care about $\langle x,x' \rangle$
\TODO{} verify calculations in 56:30 (might just be an extra)

When is Taylor expression good?
If the hessian has bounded eigenvalue (\TODO{} revisit lectures 2-3)

\section{Resources}
\begin{itemize}
    \item 
\end{itemize}


\end{document}
