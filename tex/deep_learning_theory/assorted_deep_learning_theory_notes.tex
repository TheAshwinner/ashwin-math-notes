\documentclass[answers,12pt]{exam}
\usepackage[utf8]{inputenc, xcolor}
\usepackage{hyperref}
\usepackage{amsmath,amsthm,amssymb,empheq,amsfonts,amssymb}
\usepackage{float}
\usepackage{enumitem}
\usepackage[]{mdframed}
\usepackage{todonotes}
\usepackage[normalem]{ulem}


\title{Assorted Deep Learning Theory Notes}
\author{Ashwin Sreevatsa}
\date{April 2023}

\begin{document}

\maketitle

\section*{Quick Remarks}
This doc contains some assorted notes on deep learning theory related topics.
Eventually I might merge this with the `neural\_tangent\_kernel.tex' file that I have.

\section{Linear algebra}
\subsection{Norms}
Given some linear operation $A: \mathbf{R}^m \to \mathbf{R}^n$, a question you might ask is whether there is any way to formalize a notion of `size' of an operation.
It seems like there are ways to formalize `size' for vectors (consider the different types of vector norms), so it's natural to ask if there is anything analogous for operators.

This is where norms on linear operations come in.
I'll introduce two types of norms (Frobenius norms and operator norms) and show some relationships between the two.

I'll start with the Frobenius norm.
It might be easiest to describe it by directly providing the formula and calculating it:
for some linear operation $A: \mathbf{R}^m \to \mathbf{R}^n$ with entry $A_{ij}$ corresponding to the $i$th row and $j$th column, the Frobenius norm is the following:
\[
    ||A|| = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n} |A_{ij}|^2 }    
\]
So for example, we have the following:
\[
    \begin{Vmatrix}\begin{bmatrix} 4 & -2 \\ 3 & 1 \\ \end{bmatrix} \end{Vmatrix}_{Frob}
    = \sqrt{|4|^2 + |-2|^2 + |3|^2 + |1|^2}
    = \sqrt{30}
\]
This does seem to capture some notion of `size': 
in general, the more we might expect a linear operation to affect a vector that it operates on (as in, the farther $A \vec{v}$ is from $\vec{v}$ for some $\vec{v} \in \mathbf{R}^m$), the larger the absolute value of the entries of the matrix, and therefore the larger the Frobenius norm.

However, this norm has some problems: namely, it doesn't say much about the linear operation itself.
For example, we can rearrange the values of any matrix and get the same norm, despite the resulting linear operation having very different effects on the vector.

\todo{Add frobenius norm, operator norms}

\subsection{Matrix decomposition}

\subsubsection{Eigenvalue decomposition}
We have some square matrix $A \in \mathbf{R}^{n \times n}$.
The eigenvalue decomposition of a matrix factors out $A = U \Lambda U^{-1}$, where $\Lambda$ is a diagonal matrix where the $i$th entry corresponds to the $i$th eigenvalue of the $i$th eigenvector of $A$, and $U$ has each $i$th column being the $i$th unitary eigenvector.
(Note that unitary vector means the vector that has length 1.)

To put this in another way, let's say we know the unitary eigenvectors and eigenvalues of matrix $A$: $A \vec{u}_1 = \lambda_1 \vec{u}_1, \dots, A \vec{u}_n = \lambda_1 \vec{u}_n$.
Then we can say that 
\[
    AU = A [\vec{u}_1, \dots, \vec{u}_n] = [\vec{u}_1, \dots, \vec{u}_n]
    \begin{bmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda_n \\
    \end{bmatrix}
    = U \Lambda
\]
We then have that $A = U \Lambda U^{-1}$.
Note that eigenvalue decomposition only works when $A$ is a diagonlizable matrix.

\todo{How to compute?}

\subsubsection{Singular Value Decomposition}
The goal SVD is to find some set of orthonormal vectors $\vec{v}_1, \dots, \vec{v}_n$ such that $A \vec{v}_1, \dots, A \vec{v}_n$ are orthogonal.
This is useful because eventually, we will be able to decompose $A = U \Sigma V^{\top}$ where $U$ and $V$ are both orthogonal matrices and $\Sigma$ is a diagonal matrix (though not necessarily square).
This greatly simplifies many types of computations, such as those involving $A^{\top}A, AA^{\top}$.
(For example, $A^{\top}A = U \Sigma V^{\top} V \Sigma^{\top} U^{\top} = U \Sigma \Sigma^{\top} U^{\top}$.
Also note that ${(A^{\top}A)}^k = {(U \Sigma \Sigma^{\top} U^{\top})}^k = U {(\Sigma \Sigma^{\top})}^k U^{\top}$.)

One way to look at SVD is that to perform some linear operation $A$ on some vector $\vec{w}$, we instead perform 3 other operations:
one rotation/reflection/rotoreflection, one operation scaling the vector, and then one more rotation/reflection/rotoreflection.
\missingfigure[figwidth=10cm]{Add image in img folder}

The SVD for a matrix always exists.
(There is a theorem for this \todo{Add thm})

Another property: if $A$ is square, real, and symmetric, then the eigenvalues and singular values are the same. (\todo{ver-ify})

\todo{how to com-pute}

\subsubsection{Moore-Penrose Inverse}
\todo{finish}
List of questions I'll need to resolve to be more comfortable with the Moore-Penrose Inverse:
\begin{itemize}
    \item When should you use MPI, when shouldn't you use it?
    Examples?
    \item How do we compute it?
    \item I've seen multiple formalizations (for example, the conditions, the defintion based off SVD, the limit definition).
    How do we show that all of these formalizations are equivalent?
\end{itemize}

There are 4 properties the Moore-Penrose Inverse $A^{\dagger}$ of a matrix $A$ needs to satisfy:
\begin{align*}
    1. A^{\dagger} A A^{\dagger} &= A^{\dagger} \\
    2. A A^{\dagger} A &= A \\
    3. {(A A^{\dagger})}^{\top} &= A A^{\dagger} \\
    4. {(A^{\dagger} A)}^{\top} &= A^{\dagger} A
\end{align*}

\textbf{Existence:} 
The pseudoinverse must exist for any matrix because it's constructed from the singular value decomposition, which is itself guaranteed to exist.

\textbf{Uniqueness:} 
The pseudoinverse of a matrix is unique.
Let's say that $A_1^{\dagger}, A_2^{\dagger}$ are both pseudo-inverses of $A$.
Then, we have the following:
\begin{solution}
    Uniqueness of pseudoinverse:
    \begin{align*}
        A_1^{\dagger} &= A_1^{\dagger} A A_1^{\dagger} \\
        &= A_1^{\dagger} (A A_2^{\dagger} A) A_1^{\dagger} \\
        &= A_1^{\dagger} {(A A_2^{\dagger})}^{\top} {(A A_1^{\dagger})}^{\top} \\
        &= A_1^{\dagger} {(A_2^{\dagger})}^{\top} A^{\top} {(A_1^{\dagger})}^{\top} A^{\top} \\
        &= A_1^{\dagger} {(A_2^{\dagger})}^{\top} A^{\top} \\
        &= A_1^{\dagger} {(A A_2^{\dagger})}^{\top} \\
        &= A_1^{\dagger} A A_2^{\dagger} \\
        &= {(A_1^{\dagger} A)}^{\top} A_2^{\dagger} \\
        &= A^{\top} {(A_1^{\dagger})}^{\top} A_2^{\dagger} \\
        &= {(A A_2^{\dagger} A)}^{\top} {(A_1^{\dagger})}^{\top} A_2^{\dagger} \\
        &= A^{\top} {(A_2^{\dagger})}^{\top} A^{\top} {(A_1^{\dagger})}^{\top} A_2^{\dagger} \\
        &= {(A_2^{\dagger} A)}^{\top} A^{\top} {(A_1^{\dagger})}^{\top} A_2^{\dagger} \\
        &= A_2^{\dagger} A {(A_1^{\dagger} A)}^{\top} A_2^{\dagger} \\
        &= A_2^{\dagger} A A_1^{\dagger} A A_2^{\dagger} \\
        &= A_2^{\dagger} A A_2^{\dagger} \\
        &= A_2^{\dagger}
    \end{align*}
\end{solution}

\section{Linear Regression}

\section{Kernels}
The motivation for using kernels in machine learning is to continue to use linear modeling techniques (linear regression, maximum margin classifiers, etc) on problems that don't appear to be linear.
The reason why we would want this at all is that often these linear modeling techniques are easy to compute, simple, and often have closed-form solutions.

The basic setup for kernels is the following:
for simplicity, let's say we have some regression problem where we have some set of data points $D= \{(x_1,y_1),\dots, (x_k,y_k)\}$  with $x_i \in \mathbf{R}^m$ as the input and $y_i \in \mathbf{R}$ as the output.
We want to find some function $f: \mathbf{R}^m \to \mathbf{R}$ to minimize the following loss function: $\frac{1}{2}\sum_{i=1}^{k}{(f(x_i)-y_i)}^2$.
Ideally, we might want to use some linear function to model $f$ such as $f(x) = w^{\top}x$.
However, if the data itself isn't linear, then trying to fit a linear function onto it will result in a poor performance.

The key idea here is to introduce a mapping function $\phi(x_i): \mathbf{R}^m \to \mathbf{R}^p$, where $p>m$ in the hope that in some higher dimension this will become a linear problem. \todo{Make this more concrete.}

The goal is to learn a new $f: \mathbf{R}^p \to \mathbf{R}$ that would instead minimize
\[
    \frac{1}{2}\sum_{i=1}^{k}{(f(\phi(x_i))-y_i)}^2
\]

By itself, this is an interesting idea, but one of the problems is that it might be much more computationally inefficient to compute this in $\mathbf{R}^p$ instead of $\mathbf{R}^m$. 
Indeed, it turns out that there are useful mapping functions that technically map a data point into an infinite dimensional space: such $\phi(x_i)$ would be difficult to do much with by itself.

This is where the kernel trick comes in:
instead of actually computing $\phi(x_i)$ itself, there are often ways to reformulate a problem so that the only computation being done is something of the form $\phi(x_i)^{\top}\phi(x_i)$, which often has a closed form solution. \todo{give an example}.

\subsection{Kernelized regularized sum-of-squares error}
(Borrowed from Bishop: Pattern Recognition and Machine Learning 2006).


\section{Gaussian processes}
Gaussian processes are a tool to model data by both making predictions and providing distributions over those predictions.
These notes will likely only cover the application of Gaussian processes to regression problems, but apparently there are ways to apply it to other problems (e.g.\ classification).

Here is an example of a what the result of a Gaussian process might look like:

\missingfigure[figwidth=10cm]{Add image of Gaussian process}

The following is a potential use case for using Gaussan processes in a regression problem:
consider you are a research lab attempting to design a new form of cement to better withstand stress.
You want to test out how the cement performs given different proportions of some secret mixture A added to the cement.
However the process of testing out each new cement mixture is expensive so you only want to test out with mixtures that are likely to perform well.
You can use Gaussian processes both to predict which mixtures are successful as well as model your own uncertainty to only test the mixtures that you are most uncertain about.

\missingfigure[figwidth=10cm]{Add image of how to use Gaussian processes for }

\subsection{Diving in}
Some quick background notes:
Gaussian distributions are preserved over normalization, marginalization, conditioning, multiplication.\todo{prove}
Bayes' theorem says that the conditional probability of event $Y$ given event $X$ is $P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$.

The goal of Gaussian processes is to get an expression for $P(y|x,D)$, or the probability of some output test data $y$ given some input test data $x$ and training data pairs $D = \{(x_1, y_1), \dots , (x_n,y_n)\}$.
Note that there is no discussion of how the data is being modeled in any way: we only care about the training data and the input test data.

Starting out though, we will assume a linear model for simplicity. \todo{Explain}
This will eventually be rectified by using kernels.
We have the following:
\[
    \begin{aligned}
        \textcolor{purple}{P(y|x,D)} = \textcolor{teal}{\int} \textcolor{red}{p(y|x,w)}\textcolor{blue}{p(w|D)}\textcolor{teal}{dw}
    \end{aligned}
\]
Note $w$ represents the weights of a linear model. \todo{How does $dw$ work}
For example, we might have a linear regression problem like the following: $y = w^{\top}x + \epsilon$ for some $x \in \mathbf{R}^n$, where $\epsilon$ represents some noise.
Notice what this integral represents: 
to find the \textcolor{purple}{probability that the output data is $y$ given input data $x$ and data $D$}, \textcolor{teal}{for every possible linear model, take a sum} of the products of the \textcolor{red}{probability that we would see output $y$ given that model $w$ and input test data $x$} and \textcolor{blue}{the probability that this is the model $w$ given the training data $D$}.
\todo{finish}

\subsection{Neural Network Gaussian Processes}

\subsection{Resources} 
\href{https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote15.html}{(Cornell Machine Learning Lecture 26: Gaussian Processes)}

\section{Neural Tangent Kernels}

\section{Resources}

\end{document}
